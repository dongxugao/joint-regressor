{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net surgery to produce a general `Graph` model\n",
    "\n",
    "Previously I've only had to deal with one input (either RGB alone, flow alone or stacked RGB & flow) which then gets propagated through a VGGNet which is unmodified apart from the final layer. However, now I want to make a new model where flow and RGB data travel through independent paths following the VGGNet architecture, then are merged later in the pipeline (e.g. after flattening). The model itself is defined in [`models.py`](http://localhost:8888/edit/models.py), so this notebook just contains code to transfer the ILSVRC weights for VGGNet16 into the two independent covnet streams, with the exception of the final layer (re-initialised from scratch) and the layer where the two inputs are merged together (new weights must be learnt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.visualize_util import to_graph\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import models\n",
    "from vggnet.upgrade_weights import upgrade_weights\n",
    "from vggnet.vgg16_keras import VGG_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = reload(models)\n",
    "solver = SGD()\n",
    "rgb_shape = (6, 224, 224)\n",
    "flow_shape = (2, 224, 224)\n",
    "regressor_outputs = 6\n",
    "init = 'glorot_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "huge = models.vggnet16_joint_reg_class_flow({\n",
    "    'images': (6, 224, 224),\n",
    "    'flow': (2, 224, 224),\n",
    "    'left': (16,),\n",
    "    'right': (16,),\n",
    "    'head': (16,),\n",
    "    'class': (4,)\n",
    "}, solver, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(to_graph(huge, show_shape=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow_seq = huge.nodes['flow_conv']\n",
    "rgb_seq = huge.nodes['rgb_conv']\n",
    "# Was considering using old, pre-trained weights (which are probably excellent),\n",
    "# but I think it will be interesting to see whether ordinary ILSVRC weights\n",
    "# converge better.\n",
    "# old_flow_weights_path = '/home/sam/etc/saved-keras-checkpoints/model-4272-flow-ft-old-data.h5'\n",
    "# old_rgb_weights_path = '/home/sam/etc/saved-keras-checkpoints/model-2160-rgb-ft-old-data.h5'\n",
    "ilsvrc_weights_path = './vggnet/vgg16_weights.h5'\n",
    "ilsvrc_model = VGG_16(ilsvrc_weights_path)\n",
    "upgrade_weights(flow_seq.layers, ilsvrc_model.layers)\n",
    "upgrade_weights(rgb_seq.layers, ilsvrc_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have to update the remaining convolutional layers\n",
    "front_layers = len(flow_seq.layers)\n",
    "assert front_layers == len(rgb_seq.layers), \"Flow and RGB pipelines should be same length\"\n",
    "back_ilsvrc_layers = ilsvrc_model.layers[front_layers:]\n",
    "back_seq = huge.nodes['shared_layers']\n",
    "upgrade_weights(back_seq.layers, back_ilsvrc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we can write out the weights\n",
    "huge.save_weights('vggnet/vgg16-2stream-3pose-reg-clas.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pprint import pprint\n",
    "val_samples = h5py.File('../cache/val-patches-mpii/samples-000001.h5', 'r')\n",
    "val_negs = h5py.File('../cache/val-patches-mpii/negatives.h5', 'r')\n",
    "print(val_samples.keys())\n",
    "print(val_negs.keys())\n",
    "# Ugh, looks like the negatives have totally\n",
    "# incorrect class data (I think it has had a\n",
    "# transpose or reshape taken accidentally at\n",
    "# some point)\n",
    "pprint({k: v.shape for k, v in val_negs.iteritems()})\n",
    "val_samples.close()\n",
    "val_negs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
