{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.visualize_util import to_graph\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "\n",
    "import train\n",
    "import evaluate\n",
    "from train import infer_sizes\n",
    "from models import vggnet16_regressor_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load up our data H5 and grab some trained weights for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data and get a model\n",
    "train_h5_path = '../cache/val-patches-mpii-fixed/samples-000001.h5'\n",
    "val_h5_path = '../cache/val-patches-mpii/samples-000001.h5'\n",
    "val_neg_h5_path = '../cache/val-patches-mpii/negatives.h5'\n",
    "train_h5 = h5py.File(train_h5_path, 'r')\n",
    "val_h5 = h5py.File(val_h5_path, 'r')\n",
    "val_neg_h5 = h5py.File(val_neg_h5_path, 'r')\n",
    "images = train_h5['images']\n",
    "joints = train_h5['joints']\n",
    "flow = train_h5['flow']\n",
    "val_images, val_flow, val_joints = val_h5['images'], val_h5['flow'], val_h5['joints']\n",
    "val_neg_images, val_neg_flow = val_neg_h5['images'], val_neg_h5['flow']\n",
    "input_shape, reg_outs, class_outs = infer_sizes(train_h5_path, use_rgb=True, use_flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0001, nesterov=True, momentum=0.9)\n",
    "model = vggnet16_regressor_model(input_shape, reg_outs, sgd, 'glorot_normal')\n",
    "model.load_weights('/home/sam/etc/saved-keras-checkpoints/model-rgbf-ft-old-data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by doing a basic visualisation of our model and inspecting the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print '# Data shapes'\n",
    "print 'images:', images.shape\n",
    "print 'flow:', flow.shape\n",
    "print 'joints:', joints.shape\n",
    "print 'validation images:', val_images.shape\n",
    "print 'validation flow:', val_flow.shape\n",
    "print 'validation joints:', val_joints.shape\n",
    "print\n",
    "print '# Network'\n",
    "SVG(to_graph(model, show_shape=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the training set (no predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write some functions to look at our data and also a few utilities for doing forward prop. These will be useful for inspecting activations and gradients, as well as verifying that I've written what I wanted to write to the file.\n",
    "\n",
    "Note that some of these images will look weird because they've been padded (where necessary) with their edge pixel values. This is true of the flow as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _reshape_im(im):\n",
    "    # images are stored channels-first, but numpy expects\n",
    "    # channels-last\n",
    "    return np.transpose(im, axes=(1, 2, 0))\n",
    "\n",
    "def _vis_flow(flow):\n",
    "    # clean visualisation of flow with angle of movement as\n",
    "    # hue, magnitude as saturation and a constant V of 1\n",
    "    x, y = flow\n",
    "    # normed-log makes things stand out quite a bit\n",
    "    mags = np.log(np.sqrt(x**2 + y**2) + 1)\n",
    "    norm_mags = mags / max(mags.flatten())\n",
    "    angles = (np.arctan2(x, y) + np.pi) / (2 * np.pi)\n",
    "    ones = np.ones_like(angles)\n",
    "    hsv = np.stack((angles, norm_mags, ones), axis=2)\n",
    "    return hsv_to_rgb(hsv)\n",
    "\n",
    "def _label_to_coords(label):\n",
    "    return label.reshape((-1, 2))\n",
    "\n",
    "def _plot_coords(coords):\n",
    "    # plot a label corresponding to a flattened joint vector\n",
    "    for idx, coord in enumerate(coords):\n",
    "        plt.plot(coord[0], coord[1], marker='+')\n",
    "        plt.text(coord[0], coord[1], str(idx))\n",
    "\n",
    "def show_datum(image, flow, label=None):\n",
    "    # First frame\n",
    "    im1 = _reshape_im(image[:3])\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(im1)\n",
    "    plt.axis('off')\n",
    "    plt.text(-10, -10, 'frame1')\n",
    "    \n",
    "    if label is not None:\n",
    "        coords = _label_to_coords(label)\n",
    "        first_coords = coords[:len(coords)//2]\n",
    "        _plot_coords(first_coords)\n",
    "    \n",
    "    # Second frame\n",
    "    im2 = _reshape_im(image[3:6])\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(im2)\n",
    "    plt.axis('off')\n",
    "    plt.text(-10, -10, 'frame2')\n",
    "    \n",
    "    if label is not None:\n",
    "        second_coords = coords[len(coords)//2:]\n",
    "        _plot_coords(second_coords)\n",
    "    \n",
    "    # Optical flow\n",
    "    if flow is not None:\n",
    "        im_flow = _vis_flow(flow)\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(im_flow)\n",
    "        plt.axis('off')\n",
    "        plt.text(-10, -10, 'flow')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i in np.random.permutation(len(joints))[:3]:\n",
    "    # Just visualise the input data so that I know I'm writing it out correctly\n",
    "    print 'Ground truth (NOT prediction)', i\n",
    "    show_datum(images[i], flow[i], joints[i])\n",
    "\n",
    "for i in np.random.permutation(len(val_neg_images))[:100]:\n",
    "    print 'Validation negative', i\n",
    "    show_datum(val_neg_images[i], val_neg_flow[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB+flow results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try evaluating the CNN on some of our training and evaluation data, just to see whether it's learning anything useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_on_datum(image, flow, model=model):\n",
    "    # NOTE: Need to stack data correctly and subtract flow\n",
    "    use_flow = flow is not None\n",
    "    use_rgb = image is not None\n",
    "    assert use_flow or use_rgb\n",
    "    if use_flow and use_rgb:\n",
    "        stacked = np.concatenate((image.astype('float32'), flow.astype('float32')), axis=0)\n",
    "    elif use_flow:\n",
    "        stacked = flow.astype('float32')\n",
    "    elif use_rgb:\n",
    "        stacked = image.astype('float32')\n",
    "    mp = train.read_mean_pixel('../cache/mean_pixel.mat', use_flow=use_flow, use_rgb=use_rgb)\n",
    "    stacked -= mp.reshape((len(mp), 1, 1))\n",
    "    return model.predict(stacked.reshape((1,) + stacked.shape))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# Validation images')\n",
    "# Change the [:10] up to [:100] or [:500] if you want to see more predictions, then hit\n",
    "# shift+enter to re-evaluate the cell (may need to use Kernel -> Restart & Run All if\n",
    "# the interpreter has shut down for some reason and you get an exception as a result)\n",
    "for i in np.random.permutation(len(val_joints))[:3]:    \n",
    "    print 'Validation datum', i\n",
    "    pred_joints = evaluate_on_datum(val_images[i], val_flow[i])\n",
    "    show_datum(val_images[i], val_flow[i], pred_joints)\n",
    "    \n",
    "print('# Training images')\n",
    "for i in np.random.permutation(len(joints))[:3]:\n",
    "    print 'Training datum', i\n",
    "    pred_joints = evaluate_on_datum(images[i], flow[i])\n",
    "    show_datum(images[i], flow[i], pred_joints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB-only results\n",
    "\n",
    "Here we are attempting to predict a complete half-body pose using *only* stacked RGB data from the two frames as input (no flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_input_shape, _, _ = infer_sizes(train_h5_path, use_rgb=True, use_flow=False)\n",
    "rgb_model = vggnet16_regressor_model(rgb_input_shape, reg_outs, sgd, 'glorot_normal')\n",
    "rgb_model.load_weights('../cache/keras-checkpoints/model-iter-2160-r162528.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# Validation images')\n",
    "for i in np.random.permutation(len(val_joints))[:3]:    \n",
    "    print 'Validation datum', i\n",
    "    pred_joints = evaluate_on_datum(image=val_images[i], flow=None, model=rgb_model)\n",
    "    show_datum(val_images[i], None, pred_joints)\n",
    "    \n",
    "print('# Training images')\n",
    "for i in np.random.permutation(len(joints))[:3]:\n",
    "    print 'Training datum', i\n",
    "    pred_joints = evaluate_on_datum(image=images[i], flow=None, model=rgb_model)\n",
    "    show_datum(images[i], None, pred_joints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow-only results (just predicting wrists)\n",
    "\n",
    "The opposite experiment to the previous one, where we only use flow between the two frames. This time, we're only regressing wrists, since other parts don't move as frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow_input_shape, _, _ = infer_sizes(train_h5_path, use_rgb=False, use_flow=True)\n",
    "train_wrists = train_h5['/wrists']\n",
    "val_wrists = val_h5['/wrists']\n",
    "flow_model = vggnet16_regressor_model(flow_input_shape, train_wrists.shape[1], sgd, 'glorot_normal')\n",
    "flow_model.load_weights('../cache/keras-checkpoints/model-iter-4272-r077347.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# Validation images')\n",
    "for i in np.random.permutation(len(val_wrists))[:3]:    \n",
    "    print 'Validation datum', i\n",
    "    pred_wrists = evaluate_on_datum(image=None, flow=val_flow[i], model=flow_model)\n",
    "    # Only show images so that we can see how close to GT we are;\n",
    "    # images aren't actually fed into the model\n",
    "    show_datum(val_images[i], val_flow[i], pred_wrists)\n",
    "    \n",
    "print('# Training images')\n",
    "for i in np.random.permutation(len(train_wrists))[:3]:\n",
    "    print 'Training datum', i\n",
    "    pred_wrists = evaluate_on_datum(image=None, flow=flow[i], model=flow_model)\n",
    "    # Only show images so that we can see how close to GT we are;\n",
    "    # images aren't actually fed into the model\n",
    "    show_datum(images[i], flow[i], pred_wrists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some rotations to check whether the regressor is robust to different wrist locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    # See bit.ly/1StivtV for explanation of mode parameter,\n",
    "    # which controls padding method\n",
    "    assert image.shape[0] < 10, \"Channels should be first\"\n",
    "    # transpose into channel-first form\n",
    "    transposed = image.transpose((1, 2, 0))\n",
    "    rotated = rotate(\n",
    "        transposed, angle, mode='nearest', reshape=False\n",
    "    )\n",
    "    untransposed = rotated.transpose((2, 0, 1))\n",
    "    return untransposed\n",
    "\n",
    "def mat_vec_prod(mat, vecs):\n",
    "    \"\"\"Takes an m*n transformation matrix and a p*q*n matrix\n",
    "    of n-dimensional vectors and does a matrix-vector multiply\n",
    "    between the first matrix and each vector in the second\n",
    "    matrix.\"\"\"\n",
    "    shaped_vecs = vecs.reshape((vecs.shape[0] * vecs.shape[1], vecs.shape[2]))\n",
    "    result = np.dot(mat, shaped_vecs.T).T\n",
    "    return result.reshape(vecs.shape)\n",
    "\n",
    "def rotate_flow(flow, angle):\n",
    "    rotate_tform = np.array([\n",
    "        [np.cos(angle),  np.sin(angle)],\n",
    "        [-np.sin(angle), np.cos(angle)]\n",
    "    ]).reshape((1, 1, 2, 2))\n",
    "    assert flow.shape[0] < 10, \"u, v axis should be first\"\n",
    "    rotated_mat = rotate_image(flow, angle)\n",
    "    # tanspose into channel-last form\n",
    "    trans_flow = rotated_mat.transpose((1, 2, 0))\n",
    "    # remember that flow vectors are in form [x, y], in image\n",
    "    # coordinates (so x increases from left to right and y\n",
    "    # increases from top to bottom). This is why we rotate\n",
    "    # clockwise by angle instead of anti-clockwise by angle.\n",
    "    rotated_vecs = mat_vec_prod(rotate_tform, trans_flow)\n",
    "    return rotated_vecs.transpose((2, 0, 1))\n",
    "    \n",
    "# Test rotations for RGB\n",
    "angle = 90\n",
    "test_image = images[1029, :3, :, :]\n",
    "plt.subplot(121)\n",
    "plt.imshow(test_image.transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(rotate_image(test_image, angle).transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Test rotations for flow\n",
    "test_flow = flow[1029]\n",
    "rotated_flow = rotate_flow(test_flow, angle)\n",
    "plt.subplot(121)\n",
    "plt.imshow(_vis_flow(test_flow))\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(_vis_flow(rotated_flow))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# Validation images')\n",
    "for i in np.random.permutation(len(val_wrists))[:3]:    \n",
    "    deg = np.random.choice([-90, -60, 60, 90])\n",
    "    print 'Validation datum', i, '(rotated CCW by {} deg)'.format(deg)\n",
    "    rot_flow = rotate_flow(val_flow[i], deg)\n",
    "    rot_im = rotate_image(val_images[i], deg)\n",
    "    pred_wrists = evaluate_on_datum(image=None, flow=rot_flow, model=flow_model)\n",
    "    # Only show images so that we can see how close to GT we are;\n",
    "    # images aren't actually fed into the model\n",
    "    show_datum(rot_im, rot_flow, pred_wrists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "### RGB-only model (PCP, pixel threshold accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_acc(thresholds, accs, plot_title, labels):\n",
    "    if not isinstance(accs, list) and accs.ndim == 1:\n",
    "        accs_list = [accs]\n",
    "        label_list = [labels]\n",
    "    else:\n",
    "        accs_list = np.array(accs).T\n",
    "        label_list = labels\n",
    "    for acc, label in zip(accs_list, label_list):\n",
    "        plt.plot(thresholds, acc, label=label)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlim((min(thresholds), max(thresholds)))\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('threshold (px)')\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "test_in_vals = np.linspace(0, 50, 100)\n",
    "plot_acc(test_in_vals, -1 / (test_in_vals + 1) + 1, 'Example plot', 'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp_path = '../cache/mean_pixel.mat'\n",
    "all_predictions = evaluate.get_predictions(rgb_model, mp_path, images=val_images, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'shoulder 1',\n",
    "    'elbow 1',\n",
    "    'wrist 1',\n",
    "    'shoulder 2',\n",
    "    'elbow 2',\n",
    "    'wrist 2'\n",
    "]\n",
    "all_gts = evaluate.label_to_coords(np.array(val_joints))\n",
    "thresholds = np.linspace(0, 70, 100)\n",
    "accs = evaluate.score_predictions_acc(all_gts, all_predictions, thresholds)\n",
    "plot_acc(thresholds, accs, 'RGB model accuracy (all joints)', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limbs = [\n",
    "    (0, 1),\n",
    "    (1, 2),\n",
    "    (3, 4),\n",
    "    (4, 5)\n",
    "]\n",
    "limb_names = [\n",
    "    'uarm1',\n",
    "    'farm1',\n",
    "    'uarm2',\n",
    "    'farm2'\n",
    "]\n",
    "\n",
    "pcps = evaluate.score_predictions_pcp(all_gts, all_predictions, limbs)\n",
    "\n",
    "def show_pcp(pcps, limb_names):\n",
    "    fmt = lambda s: '{:>20}'.format(s)\n",
    "    print(fmt('limb name') + ''.join(fmt(l) for l in limb_names))\n",
    "    print(fmt('pcp') + ''.join(fmt(p) for p in pcps))\n",
    "    \n",
    "show_pcp(pcps, limb_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow-only model (pixel threshold accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flow_predictions = evaluate.get_predictions(flow_model, mp_path, flows=val_flow, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow_labels = [\n",
    "    'wrist 1',\n",
    "    'wrist 2'\n",
    "]\n",
    "# There's no PCP here because we only have disconnected joints\n",
    "flow_accs = evaluate.score_predictions_acc(all_gts[:, (2, 5)], flow_predictions, thresholds)\n",
    "plot_acc(thresholds, flow_accs, 'Flow model accuracy (wrist only)', flow_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
