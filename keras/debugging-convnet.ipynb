{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential, Graph, model_from_json\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.visualize_util import to_graph\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "import train\n",
    "import evaluate\n",
    "from train import infer_sizes\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load up our data H5 and grab some trained weights for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data and get a model\n",
    "train_h5_path = '../cache/train-patches-mpii/samples-000001.h5'\n",
    "train_neg_h5_path = '../cache/train-patches-mpii/negatives.h5'\n",
    "val_h5_path = '../cache/val-patches-mpii/samples-000001.h5'\n",
    "val_neg_h5_path = '../cache/val-patches-mpii/negatives.h5'\n",
    "train_h5 = h5py.File(train_h5_path, 'r')\n",
    "train_neg_h5 = h5py.File(train_neg_h5_path, 'r')\n",
    "val_h5 = h5py.File(val_h5_path, 'r')\n",
    "val_neg_h5 = h5py.File(val_neg_h5_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images, train_flow = train_h5['images'], train_h5['flow']\n",
    "train_neg_images, train_neg_flow = train_neg_h5['images'], train_neg_h5['flow']\n",
    "val_images, val_flow = val_h5['images'], val_h5['flow']\n",
    "val_neg_images, val_neg_flow = val_neg_h5['images'], val_neg_h5['flow']\n",
    "ds_shape = infer_sizes(train_h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0001, nesterov=True, momentum=0.9)\n",
    "model = models.vggnet16_joint_reg_class_flow(ds_shape, sgd, 'glorot_normal')\n",
    "model.load_weights('../cache/kcnn-flow-rgb-tripose-from-3840-plus-1024/model-iter-10240-r181250.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by doing a basic visualisation of our model and inspecting the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print '# Data shapes'\n",
    "print 'images:', train_images.shape\n",
    "print 'flow:', train_flow.shape\n",
    "print 'validation images:', val_images.shape\n",
    "print 'validation flow:', val_flow.shape\n",
    "print\n",
    "print '# Network'\n",
    "SVG(to_graph(model, show_shape=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upgraded_model = models.upgrade_multipath_vggnet(model)\n",
    "upgraded_model.compile(sgd, model.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVG(to_graph(upgraded_model, show_shape=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and reloading models\n",
    "\n",
    "Need to check that my convert to fully convolutional network --> save model to JSON and weights to HDF5 --> reload model from Matlab pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_json = upgraded_model.to_json()\n",
    "with open('../cache/cnn_model.json', 'w') as fp:\n",
    "    fp.write(mod_json)\n",
    "upgraded_model.save_weights('../cache/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../cache/cnn_model.json') as fp:\n",
    "    json_data = fp.read()\n",
    "m2 = model_from_json(json_data)\n",
    "m2.load_weights('../cache/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the training set (no predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write some functions to look at our data and also a few utilities for doing forward prop. These will be useful for inspecting activations and gradients, as well as verifying that I've written what I wanted to write to the file.\n",
    "\n",
    "Note that some of these images will look weird because they've been padded (where necessary) with their edge pixel values. This is true of the flow as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids = loadmat('../cache/centroids.mat')['centroids'][0].tolist()\n",
    "\n",
    "def _label_to_coords(label):\n",
    "    return label.reshape((-1, 2))\n",
    "\n",
    "def get_centroids(classes, centroids):\n",
    "    \"\"\"Grab the centroid of the cluster associated with each classifier output.\n",
    "    \n",
    "    :param classes: One-of-K vector (N-first, K-second) indicating appropriate\n",
    "                    centroids\n",
    "    :param centroids: list of P elements, each of which is a CxJp array, where\n",
    "                      P is the number of poselets, C is the number of clusters\n",
    "                      per poselet, and Jp is the number of regressor outputs per\n",
    "                      cluster\n",
    "    :returns: N-element list of the form [(class, poselet, centroid)], where\n",
    "              class is in [0, P] (0 is background), centroid is None (for\n",
    "              background) or a Jp/2x2 array giving (x, y) locations for each\n",
    "              joint, and poselet is a poselet index in [0, C)\"\"\"\n",
    "    assert classes.ndim == 2, \"Expect one-of-K classes!\"\n",
    "    class_nums = np.argmax(classes, axis=1)\n",
    "    classes_per_poselet = (classes.shape[1] - 1) / len(centroids)\n",
    "    rv = []\n",
    "    \n",
    "    for num in class_nums:\n",
    "        if num == 0:\n",
    "            rv.append((0, None, None))\n",
    "            continue\n",
    "        poselet_class_idx = (num - 1) % classes_per_poselet\n",
    "        poselet = (num - 1) // classes_per_poselet\n",
    "        centroid = centroids[poselet][poselet_class_idx]\n",
    "        # Will have to fix this later, if I change number of\n",
    "        # points per poselet\n",
    "        assert centroid.shape == (16,)\n",
    "        rv.append((\n",
    "            poselet + 1, poselet_class_idx, _label_to_coords(centroid)\n",
    "        ))\n",
    "\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _reshape_im(im):\n",
    "    # images are stored channels-first, but numpy expects\n",
    "    # channels-last\n",
    "    return np.transpose(im, axes=(1, 2, 0))\n",
    "\n",
    "def _vis_flow(flow):\n",
    "    # clean visualisation of flow with angle of movement as\n",
    "    # hue, magnitude as saturation and a constant V of 1\n",
    "    x, y = flow\n",
    "    # normed-log makes things stand out quite a bit\n",
    "    mags = np.log(np.sqrt(x**2 + y**2) + 1)\n",
    "    norm_mags = mags / max(mags.flatten())\n",
    "    angles = (np.arctan2(x, y) + np.pi) / (2 * np.pi)\n",
    "    ones = np.ones_like(angles)\n",
    "    hsv = np.stack((angles, norm_mags, ones), axis=2)\n",
    "    return hsv_to_rgb(hsv)\n",
    "\n",
    "def _plot_coords(coords):\n",
    "    # plot a label corresponding to a flattened joint vector\n",
    "    for idx, coord in enumerate(coords):\n",
    "        plt.plot(coord[0], coord[1], marker='+')\n",
    "        plt.text(coord[0], coord[1], str(idx))\n",
    "\n",
    "def show_datum(image, flow, label=None):\n",
    "    # First frame\n",
    "    im1 = _reshape_im(image[:3])\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(im1)\n",
    "    plt.axis('off')\n",
    "    plt.text(-10, -10, 'frame1')\n",
    "    \n",
    "    if label is not None:\n",
    "        coords = _label_to_coords(label)\n",
    "        first_coords = coords[:len(coords)//2]\n",
    "        _plot_coords(first_coords)\n",
    "    \n",
    "    # Second frame\n",
    "    im2 = _reshape_im(image[3:6])\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(im2)\n",
    "    plt.axis('off')\n",
    "    plt.text(-10, -10, 'frame2')\n",
    "    \n",
    "    if label is not None:\n",
    "        second_coords = coords[len(coords)//2:]\n",
    "        _plot_coords(second_coords)\n",
    "    \n",
    "    # Optical flow\n",
    "    if flow is not None:\n",
    "        im_flow = _vis_flow(flow)\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(im_flow)\n",
    "        plt.axis('off')\n",
    "        plt.text(-10, -10, 'flow')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def get_joints(fp, index, ds_order=('left', 'right', 'head')):\n",
    "    class_num = np.argmax(fp['class'][index])\n",
    "    ds_name = ds_order[(class_num-1)%3]\n",
    "    return fp[ds_name][index]\n",
    "\n",
    "for i in np.random.permutation(len(train_images))[:0]:\n",
    "    # Just visualise the input data so that I know I'm writing it out correctly\n",
    "    print 'Training ground truth (NOT prediction)', i\n",
    "    j = get_joints(train_h5, i)\n",
    "    show_datum(train_images[i], train_flow[i], j)\n",
    "    for ds in ('left', 'right', 'head'):\n",
    "        jx = train_h5[ds][i]\n",
    "        print('{}: {}'.format(ds, jx))\n",
    "    print('Class: {}'.format(train_h5['class'][i]))\n",
    "    \n",
    "for i in np.random.permutation(len(train_neg_images))[:0]:\n",
    "    # Just visualise the input data so that I know I'm writing it out correctly\n",
    "    print 'Training negative', i\n",
    "    show_datum(train_neg_images[i], train_neg_flow[i])\n",
    "    for ds in ('left', 'right', 'head'):\n",
    "        jx = train_neg_h5[ds][i]\n",
    "        print(jx.shape)\n",
    "        # print('{}: {}'.format(ds, jx))\n",
    "    print('Class: {}'.format(train_neg_h5['class'][i]))\n",
    "    \n",
    "# for i in np.random.permutation(len(val_joints))[:3]:\n",
    "#     print 'Validation ground truth (NOT prediction)', i\n",
    "#     show_datum(val_images[i], val_flow[i], val_joints[i])\n",
    "\n",
    "# for i in np.random.permutation(len(val_neg_images))[:3]:\n",
    "#     print 'Validation negative', i\n",
    "#     show_datum(val_neg_images[i], val_neg_flow[i])\n",
    "\n",
    "train_centroid_classes = train_h5['poselet']\n",
    "train_centroids = get_centroids(train_centroid_classes[:], centroids)\n",
    "for i in np.random.permutation(len(train_images))[:100]:\n",
    "    print 'Training ground truth poselet (NOT prediction)', i\n",
    "    cls, pslt, coords = train_centroids[i]\n",
    "    show_datum(train_images[i], train_flow[i], coords)\n",
    "    true_cls = np.argmax(train_h5['class'][i])\n",
    "    assert true_cls == cls, '%i (true) vs. %i (from gc)' % (true_cls, cls)\n",
    "    print('Class: {}, poselet: {}\\n\\n\\n'.format(true_cls, pslt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint regressor results\n",
    "\n",
    "This regressor takes in both flow and RGB data, pushes them through different streams, then outputs predictions for each of 3 poselets, along with a confidence for which poselet is in the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try evaluating the CNN on some of our training and evaluation data, just to see whether it's learning anything useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_on_datum(data, model=model):\n",
    "    mps = train.read_mean_pixels('../cache/mean_pixel.mat')\n",
    "    data = train.sub_mean_pixels(mps, data)\n",
    "    return model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_random_samples(images, flow, true_classes, num_samples, title='Sample'):\n",
    "    for i in np.random.permutation(len(images))[:num_samples]:\n",
    "        print('\\n\\n\\n{} {}'.format(title, i))\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = evaluate_on_datum({\n",
    "            'images': images[i:i+1], 'flow': flow[i:i+1]\n",
    "        })\n",
    "        \n",
    "        # Get class info\n",
    "        class_names = ('background', 'left', 'right', 'head')\n",
    "        tc_idx = np.argmax(true_classes[i])\n",
    "        out_probs = preds['class'][0]\n",
    "        pc_idx = np.argmax(preds['class'][0])\n",
    "        pc_prob = out_probs[pc_idx] * 100\n",
    "        print('Class confidences: {}'.format(preds['class'][0]))\n",
    "        print('True class: {}; Predicted class: {} ({}%)'.format(\n",
    "                class_names[tc_idx],\n",
    "                class_names[pc_idx], pc_prob\n",
    "        ))\n",
    "        print(u'\\u2713 Correct class' if pc_idx == tc_idx\n",
    "              else u'\\u2717 Incorrect class')\n",
    "        \n",
    "        # Visualise\n",
    "        if tc_idx > 0:\n",
    "            label = preds[class_names[tc_idx]]\n",
    "        else:\n",
    "            label = None\n",
    "        show_datum(images[i], flow[i], label=label)\n",
    "        \n",
    "        # Get error\n",
    "        pos_mask = true_classes[i].astype('bool')\n",
    "        cross_entropy = -np.log(out_probs[pos_mask]).sum() - np.log(out_probs[~pos_mask]).sum()\n",
    "        tc_name = class_names[tc_idx]\n",
    "        l1_dist = preds[class_names[tc_idx]]\n",
    "\n",
    "print('# Validation images')\n",
    "evaluate_random_samples(val_images, val_flow, val_h5['class'], 100, title='Validation datum')\n",
    "    \n",
    "print('\\n\\n\\n# Training images')\n",
    "evaluate_random_samples(train_images, train_flow, train_h5['class'], 100, title='Training datum')\n",
    "\n",
    "# These are much less interesting because the classifier is good at picking out background patches.t\n",
    "print('\\n\\n\\n# Validation negatives')\n",
    "evaluate_random_samples(val_neg_images, val_neg_flow, val_neg_h5['class'], 20, title='Validation negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "### RGB-only model (PCP, pixel threshold accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_acc(thresholds, accs, plot_title, labels):\n",
    "    if not isinstance(accs, list) and accs.ndim == 1:\n",
    "        accs_list = [accs]\n",
    "        label_list = [labels]\n",
    "    else:\n",
    "        accs_list = np.array(accs).T\n",
    "        label_list = labels\n",
    "    for acc, label in zip(accs_list, label_list):\n",
    "        plt.plot(thresholds, acc, label=label)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlim((min(thresholds), max(thresholds)))\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('threshold (px)')\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "test_in_vals = np.linspace(0, 50, 100)\n",
    "plot_acc(test_in_vals, -1 / (test_in_vals + 1) + 1, 'Example plot', 'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp_path = '../cache/mean_pixel.mat'\n",
    "all_predictions = evaluate.get_predictions(rgb_model, mp_path, images=val_images, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'shoulder 1',\n",
    "    'elbow 1',\n",
    "    'wrist 1',\n",
    "    'shoulder 2',\n",
    "    'elbow 2',\n",
    "    'wrist 2'\n",
    "]\n",
    "all_gts = evaluate.label_to_coords(np.array(val_joints))\n",
    "thresholds = np.linspace(0, 70, 100)\n",
    "accs = evaluate.score_predictions_acc(all_gts, all_predictions, thresholds)\n",
    "plot_acc(thresholds, accs, 'RGB model accuracy (all joints)', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limbs = [\n",
    "    (0, 1),\n",
    "    (1, 2),\n",
    "    (3, 4),\n",
    "    (4, 5)\n",
    "]\n",
    "limb_names = [\n",
    "    'uarm1',\n",
    "    'farm1',\n",
    "    'uarm2',\n",
    "    'farm2'\n",
    "]\n",
    "\n",
    "pcps = evaluate.score_predictions_pcp(all_gts, all_predictions, limbs)\n",
    "\n",
    "def show_pcp(pcps, limb_names):\n",
    "    fmt = lambda s: '{:>20}'.format(s)\n",
    "    print(fmt('limb name') + ''.join(fmt(l) for l in limb_names))\n",
    "    print(fmt('pcp') + ''.join(fmt(p) for p in pcps))\n",
    "    \n",
    "show_pcp(pcps, limb_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow-only model (pixel threshold accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flow_predictions = evaluate.get_predictions(flow_model, mp_path, flows=val_flow, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow_labels = [\n",
    "    'wrist 1',\n",
    "    'wrist 2'\n",
    "]\n",
    "# There's no PCP here because we only have disconnected joints\n",
    "flow_accs = evaluate.score_predictions_acc(all_gts[:, (2, 5)], flow_predictions, thresholds)\n",
    "plot_acc(thresholds, flow_accs, 'Flow model accuracy (wrist only)', flow_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
