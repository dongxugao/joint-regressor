\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{tabu}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{hyperref}
% See
% https://stackoverflow.com/questions/1061112/eliminate-space-before-beginitemize
% Lots of authors seem to hate the spacing before lists in LaTeX. nolistsep
% takes it out.
\usepackage{enumitem}
%\setlist{nolistsep}

%===========================================================
% Custom commands
\newcommand{\mat}{\mathbf}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\argmin}{arg min}

% For final copy (from WACV style)
\newif\ifaccvfinal{}
\accvfinalfalse{}
\def\accvfinalcopy{\global\accvfinaltrue}
% Here's how you enable final copy styles (be careful of the acknowledgements!)
\accvfinalcopy{}

\ifaccvfinal\else
\usepackage{ruler}
\fi

%===========================================================
\begin{document}
\pagestyle{headings}
\mainmatter{}

\ifaccvfinal\else
\def\ACCV16SubNumber{***}  % Insert your submission number here
\fi

%===========================================================
\title{Human Pose Estimation in Videos with Biposelets}
\ifaccvfinal{}
    \author{Sam Toyer}
    \institute{%
        Australian National University, Canberra, Australia\\
        \texttt{u5568237@anu.edu.au}
    }
\else
    \titlerunning{ACCV-16 submission ID \ACCV16SubNumber}
    \authorrunning{ACCV-16 submission ID \ACCV16SubNumber}

    \author{Anonymous ACCV 2016 submission}
    \institute{Paper ID \ACCV16SubNumber}
\fi

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Misc formatting notes:
%
% - Use "This is not a pipe~\cite{sourceYY}." to cite papers (not fancy natbib
%   stuff)
% - They want punctuated equations and a ~ space between the end of the equation
%   and the punctuation (either comma or full stop).
% - Don't feel obligated to go to 14 pages. A quick skim of accepted papers from
%   ACCV 2014 shows that going above 10 pages is the norm (shortest I saw was 12
%   pages), but that fewer than 14 pages of actual content is totally
%   acceptable. Edit: it turns out that 14 pages is not a lot when most of your
%   page is margin. Meh.
% - Structure is flexible. For instance:
%   - There's no need for a separate discussion section (apart from discussion
%     in experiments section)
%   - Some authors don't include a separate section for related work, some do.
%   - Long sections are acceptable (but poor practice, IMO)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tentative list of figures to produce:
%
% - One flow chart for the entire pose estimation process (re-use images from
%   PowerPoint, but in Inkscape or something that plays nicely with LaTeX)
% - Big panel of biposelets
% - Illustration of stitching model for single frames (much like the one in my
%   PowerPoint)
% - Possibly a small summary figure at the beginning showing some representative
%   detections to lend flavour to the rest of the paper.
% - Possibly a CNN architecture diagram showing how flow is merged with RGB
%   images---would prefer not to do that, since it's not a well-explored aspect
%   of my approach. If I do choose to do that, I can probably stick to boxes to
%   represent the CNN layers, with some images of RGB input and flow on one side
%   and biposelets on the other
% - Panels of good at bad predictions for MPII Cooking Activities, Poses in the
%   Wild and H3.6M
% - Plots of PCK and strict PCP:
%   - Raw PCK for Poses in the Wild
%   - PCP-strict at various thresholds for PIW (include table for PCP@0.5)
%   - PCP-strict at various thresholds for MPII Cooking Activities (also needs
%     table for PCP@0.5)
%   - PCKh for H3.6M in same format as recurrent convnet paper

\begin{abstract}
The problem of human pose estimation in still images has been well-studied in
recent years, but making effective use of the temporal information inherent in
videos is still an open problem. This paper presents a new model which is forced
to learn temporal relationships by predicting poses in several frames at a time.
The new approach caters to the detection and classification capabilities of
convolutional networks by casting pose estimation as a problem of detecting the
\textit{biposelets} which constitute a pair of poses in adjacent video frames.
Relative to existing single-frame-at-a-time methods, this new approach also
makes it simpler to combine pose predictions into a coherent sequence of poses
across an entire video. Experiments show that a biposelet approach outperforms
previous work on shoulder localisation, but that localisation of wrists remains
a challenge.
\end{abstract}

\section{Introduction}\label{sec:intro}

Human pose estimation is the task of localising the joints of a person in an
image or throughout a video sequence. Effective pose estimation must accommodate
motion blur and self-occlusion, be invariant to subject clothing and scene
clutter, and exhibit an understanding of anatomic constraints on poses; these
requirements make pose estimation a challenging problem even with
state-of-the-art computer vision techniques. In practice, 2D pose estimation is
useful as a preprocessing step for higher-level tasks, including 3D pose
estimation~\cite{andriluka2010monocular,zhou2016spatio} and action
recognition~\cite{cheron2015p,yao2011does}.

Intuitively, pose estimation in videos ought to be more effective than pose
estimation in static images. Even if a pose is blurred or occluded in one frame
of a video, it is often possible for humans to approximate it well by making use
of the context provided by surrounding frames. In practice, leveraging this
temporal redundancy is challenging. Simply feeding ``temporal features'' like
optical flow or multiple frames to a static pose estimator results in only
modest accuracy improvements~\cite{jain2014modeep}. On the other hand, extending
a model to explicitly consider joint motion can yield intractable inference
problems which require complex approximations~\cite{cherian2014mixing}.
Finding ways to exploit temporal information in an efficient an effective manner
is thus an active area of research.

The main contribution of this paper is to propose and test a new approach to
human pose estimation in videos based on the concept of ``biposelets''. As
explained in Section~\ref{sec:decomp}, a biposelet describes a configuration of
some subset of a person's joints in two adjacent video frames; this allows
biposelets to express both the position and instantaneous motion of joints.
Hence, casting pose estimation as biposelet detection problem forces the
proposed model to learn to make use of the information present in two frames of
video at a time. As an added bonus, this pairwise detection approach yields a
two poses at a time; this gives rise to a relatively simple notion of temporal
consistency of poses across a video sequence, which is elucidated in
Section~\ref{sec:stitching}.

The complete pose estimation pipeline is divided into two stages, both depicted
in Figure~\ref{fig:pipeline}: in the first stage, each pair of frames is
processed independently of the others to produce sets of candidate pose pairs.
The first stage makes use of a Convolutional Neural Network (CNN) to find
regions of each frame pair which visually resemble different biposelets,
followed by a graphical model which ensures that the relative positions of
predicted biposelets are anatomically reasonable. In the second ``stitching''
stage, a single pair of poses is chosen from each set of candidate pose pairs;
the resultant sequence of pose pairs can then be turned into a sequence of
single poses by averaging the two poses predicted for each frame.

In Section~\ref{sec:experiments}, this strategy is evaluated on three
established pose estimation benchmarks. The evaluation shows a significant
improvement in shoulder localisation accuracy compared to existing work, and
competitive performance on elbows, but lower performance on wrists.
Section~\ref{sec:discussion} makes several suggestions on how to improve the
performance of a biposelet-based model on more challenging joints.

\begin{figure*}[t]
\begin{center}
\includegraphics[draft,width=\textwidth,height=50mm]{figures/pipeline.eps}
\end{center}
\vspace{-7mm}
\caption{An illustration of the two stages of the pipeline.}
\label{fig:pipeline}
\end{figure*}

\section{Related work}\label{sec:related-work}

Pose estimation methods for static images typically build on some sort of
appearance model which can be used to determine whether a specific joint is
present in a small image patch. Linear classifiers applied to
histogram-of-gradients features are commonly used for this
purpose~\cite{yang2011articulated,cherian2014mixing}, but have been eclipsed by
increasingly sophisticated convolutional network
architectures~\cite{jain2014modeep,tompson2014joint,chen2014articulated,pfister2015flowing,wei2016convolutional},
and the CNN-based appearance model used in this work reflects this trend.

Appearance models can be complemented by global constraints on the relative
positions of joints. For instance, Yang and Ramanan~\cite{yang2011articulated}
define a graphical model which encourages joint locations to fall in regions of
the image with high appearance scores while penalising atypical limb lengths and
orientations. Several approaches also use the appearance of body parts to infer
``types'' which characterise those parts' positions relative to their
neighbours~\cite{yang2011articulated,chen2014articulated}. The approach
presented here is similar, although it breaks with past work by attempting to
simultaneously localise entire subsets of a person's joints instead of a single
joint at a time.

In videos, pose estimation is frequently accomplished using a
tracking-by-detection approach: first, a set of poses is estimated independently
for each frame of the video. Next, the estimates for each frame are combined
into a single temporally coherent
sequence~\cite{andriluka2010monocular,ferrari2008progressive,cherian2014mixing,ramanan2005strike}.
This work follows a similar pattern, but applies a detector to pairs of frames
instead of a single frame at a time. As mentioned previously, this makes it
simple to stitch predictions into a complete sequence. In contrast, past work
using the frame-at-a-time method has had to resort to more complex flow- and
appearance-based heuristics to ensure temporal consistency between predicted
poses~\cite{cherian2014mixing}.

There have been several attempts to exploit motion information at the CNN level
rather than just through stitching heuristics: MoDeep~\cite{jain2014modeep}
simply augmented a single-frame heatmap regressor for joint positions with
``motion features'' derived from neighbouring frames. For localisation of
fast-moving joints, the authors reported a boost in performance over an
equivalent model without motion features, but little boost in performance for
slower-moving joints like the elbows. Their result suggests that temporal
relationships between joint position were not being learnt effectively.

Other approaches to learning temporal relationships at the network level include
``flowing convnets''~\cite{pfister2015flowing}, and the recurrent network
approach of Fragkiadaki et al.~\cite{fragkiadaki2015recurrent}. Flowing convnets
use optical flow to warp joint position heatmaps between frames, then pool the
heatmaps at each frame to improve accuracy; despite yielding state-of-the-art
accuracy, that approach only ``learns'' temporal relationships by choosing a
small set of pooling weights for combining backwarped heatmaps. In contrast,
Fragkiadaki et al.\ attempt to learn temporal relationships directly using a
recurrent neural network architecture. Both approaches are benchmarked in
Section~\ref{sec:experiments}.

\section{Detecting pose pairs}

\subsection{Subposes and biposelets}\label{sec:decomp}

The first stage of the pose estimation pipeline inspects two video frames at a
time and infers a set of poses which may be present in each. For the purposes of
this stage, a pose is decomposed into a fixed set of subposes, each of which
correspond to a subset of joints in the original pose. Subposes are chosen so
that each subpose shares exactly one joint with neighbouring subposes, and so
that each joint in the original pose is present in at least one subpose. For
instance, a pose containing shoulder, elbow and wrist joints could be decomposed
into one subpose containing wrist and elbow joints on the left side of the body,
one subpose containing elbow and shoulder joints on the left hand side, one
subpose containing both the left and right shoulders, etc.

Rather than representing the locations of each joint in each subpose directly,
this stage discretises the space of configurations for joints within each
subpose into $K$ \textit{biposelets}. Biposelets are so named in analogy to the
\textit{poselets} of Bourdev and Malik~\cite{bourdev2009poselets}. Unlike their
namesakes, however, biposelets define positions of joints in two frames rather
than just one. This allows biposelets to represent both the relative positions
of joints and their movement over time. A representative set of biposelets
learnt for the MPII Cooking Activities pose estimation
dataset~\cite{rohrbach2012database} is shown in Figure~\ref{fig:biposelets}.

\begin{figure*}[t]
\begin{center}
\begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}}
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-1-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-2-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-3-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-4-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-5-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-6-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-230/sample-7-f0.jpg}\\
%
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-1-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-2-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-3-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-4-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-5-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-6-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-289/sample-7-f0.jpg}\\
%
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-1-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-2-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-3-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-4-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-5-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-6-f0.jpg}\,&
\includegraphics[height=0.10\linewidth]{figures/biposelets/poselet-297/sample-7-f0.jpg}\\
\end{tabular}
\end{center}
\vspace{-7mm}
\caption{Biposelets learnt for left elbows in the MPII Cooking Activities
dataset. Different rows show different biposelets. For brevity,
only the first frame associated with each biposelet is shown.}
\label{fig:biposelets}
\end{figure*}

% TODO: Give motivation for decomposing into subposes and biposelets instead of:
% 1) Considering a single joint at a time
% 2) Considering the entire pose

\subsection{Frame pair model}

Formally, the frame pair model conceptualises a pose as a tree-structured graph
$\mathcal G = (\mathcal S, \mathcal E)$ consisting of a set of subposes
$\mathcal S$ and a set of edges $\mathcal E \subset \mathcal S \times \mathcal
S$. Subposes are chosen according to the constraints listed in
Section~\ref{sec:decomp}, and a pair of subposes $(s_1, s_2)$ will have an edge
between them iff they have a joint in common. The formal objective of the first
stage of the pipeline is to take a pair of frames $(\mat I_1, \mat I_2)$ and
output, for each subpose $s$, a subpose location $\vec l_s \in \mathbb R^2$
within the frame pair and a biposelet type $t_s \in \{1, \ldots, K\}$.
Specifically, the first stage must find some $\mat L = \begin{bmatrix}\vec l_1 &
\vec l_2 & \cdots & \vec l_{|\mathcal S|}\end{bmatrix}^T$ and $\vec t =
\begin{bmatrix}t_1 & t_2 & \cdots & t_{|\mathcal S|}\end{bmatrix}$ which
minimises the cost
%
\begin{equation}\label{eqn:full-cost}
C(\mat L, \vec t \mid \mat D_{12})
= w_0 + \sum_{s \in \mathcal S} \phi_s(\vec l_s, t_s; \mat D_{12})
+ \sum_{(s_1, s_2) \in \mathcal E}
    \psi_{s_1 s_2}(\vec l_{s_1}, \vec l_{s_2}, t_1, t_2)~,
\end{equation}
%
where $w_0$ is a bias, each $\phi_s(\cdot)$ is an appearance term, each
$\psi_{s_1,s_2}$ is a pairwise cost, and $\mat D_{12}$ is the concatenation
of the frames $\mat I_1$ and $\mat I_2$.

Recall that each pair of neighbouring subposes has a single joint in common;
the aim of the pairwise terms is to ensure that locations of neighbouring
subposes are chosen so that predicted locations for their shared joints are
consistent. Concretely, each pairwise cost can be written out in full as
%
\begin{equation}\label{eqn:pair-cost}
\psi_{s_1 s_2}(\vec l_{s_1}, \vec l_{s_2}, t_1, t_2)
= \langle
    \vec w_{s_1 s_2},
    \vec \Delta(\vec l_{s_1} - \vec l_{s_2}  - \vec m_{t_1 t_2})
\rangle~,
\end{equation}
%
where $\vec \Delta(\begin{bmatrix}\delta_x & \delta_y\end{bmatrix}) =
\begin{bmatrix}\delta_x^2 & \delta_x & \delta_y^2 & \delta_y\end{bmatrix}$ is a
deformation feature, $\vec w_{s_1 s_2}$ is a learnt weight vector, and
$\vec m_{t_1 t_2}$ is an ``ideal'' displacement between $s_1$ and $s_2$. If the
joint shared between $s_1$ and $s_2$ is at location $\vec r_{t,f}$ in frame $f$
and biposelet $t$ (relative to the centre of the subpose), then $\vec m_{t_1
t_2}$ is simply
\begin{equation}
\frac{%
    \left(\vec r_{t_2,1} - \vec r_{t_1,1}\right)
    + \left(\vec r_{t_2,2} - \vec r_{t_1,2}\right)
}{2}~.
\end{equation}

Each appearance term $\phi_s(\vec l_s, t_s; \mat D_{12})$ reflects the degree to
which a small region around the location $\vec l_s$ looks like a subpose $s$
with biposelet type $t_s$. Concretely,
\begin{equation}\label{eqn:unary-cost}
\phi_s(\vec l_s, t_s; \mat D_{12}) = -w_s \log p(s, t_s \mid \mat D_{12}(
\vec l_s))~,
\end{equation}
%
where $w_s$ is a learnt weight and $\mat D_{12}(\vec l_s)$ denotes a crop of the
frame pair $(\mat I_1, \mat I_2)$ and the optical flow between them at location
$\vec l_s$. $p(s, t \mid \mat D(\vec l))$ is the probability that the image
patch $\mat D(\vec l)$ contains a subpose $s$ with biposelet $t$, with the
special values $(s, t) = (0, 0)$ denoting a background patch with no subpose.

$p(s, t \mid \mat D(\vec l))$ is produced by a two-stream convolutional neural
network with an architecture loosely following the 16 layer architecture of
Simonyan et al.~\cite{simonyan2014very}. Specifically, Simonyan et al.'s
original single-stream architecture has been split in two, with one stream
processing a stacked pair of RGB video frames and the other processing the flow;
the two streams are merged by concatenating them after the third pooling layer.
The output is a probability distribution over all subposes $s$ and biposelets
$t$ for each subpose, with the aforementioned special values of $(s, t) = (0,
0)$ denoting the background class.

At test time, the network can be used as an efficient sliding window detector by
converting the final dense layers to convolutions, which yields a fully
convolutional network~\cite{sermanet2013overfeat}. Section~\ref{sec:learning}
discusses training-time considerations.

\subsection{Inference on frame pair model}

The full cost (\ref{eqn:full-cost}) can be minimised efficiently by exploiting
the subpose graph's tree structure. For each subpose $s$, define the minimal
cost of the subtree rooted at $s$ as
%
\begin{equation}\label{eqn:reccost}
\begin{split}
M(\vec l_s, t_s; \mat D_{12}) =
&\sum_{s' : \pa(s') = s} \min_{\vec l_{s'}, t_{s'}} \left(\psi_{s s'}(\vec l_s,
\vec l_{s'}, t_s, t_{s'}) + M(\vec l_{s'}, t_{s'}; \mat D_{12})\right)
\\&\quad+ \phi_s(\vec l_s, t_s; \mat D_{12})~,
\end{split}
\end{equation}
%
where $\vec l_s$ and $t_s$ are the location and biposelet type, respectively, of
$s$, and $\pa(s') = s$ iff subpose $s$ is the parent of subpose $s'$ in
$\mathcal G$.

$M$ can be calculated for each location and type of the root subpose by
proceeding from the leaves of $\mathcal G$ upwards: for each leaf subpose $s_l$,
$M(\vec l_{s_l}, t_{s_l}; \mat D_{12})$ is simply $\phi_{s_l}(\vec l_{s_l},
t_{s_l}; \mat D_{12})$. For a non-leaf subpose $s$, $M(\vec l_s, \vec t_s; \mat
D_{12})$ can be computed by first evaluating $M(\vec l_{s_c}, t_{s_c}; \mat
D_{12})$ for each child $s_c$ of $s$, then applying Felzenszwalb and
Huttenlocher's distance transform technique~\cite{felzenszwalb2012distance} to
find the $\vec l_{s_c}$ which minimises $\psi_{s s_c}(\vec l_s, \vec l_{s_c},
t_s, t_{s_c})$ for each $t_{s_c}$.

If there are $N$ locations in the image and $K$ possible biposelet types, then
applying this procedure to a non-leaf node will take $\mathcal O(N K^2)$ time
for each child and each $\vec l_s$. Repeating the procedure for all subposes in
the subpose graph $\mathcal G$ thus takes $\mathcal O(|\mathcal S| N K^2)$ time.
Intuitively, this means that introducing new subposes to $\mathcal S$ or scaling
up the number of pixels $N$ is ``cheap'', but increasing the number of biposelet
types drives up computational cost rapidly.

Having evaluated (\ref{eqn:reccost}) for each possible root location $\vec
l_{s_R}$ and root type $t_{s_R}$, finding the pose configuration which minimises
(\ref{eqn:full-cost}) is a simple matter of looking up
%
\begin{equation}
\argmin_{\vec l_{s_R}, t_{s_R}} M(\vec l_{s_R}, \vec t_{s_R}; \mat D_{12})
\end{equation}
%
and then backtracking to recover the locations and types of all other subposes.
This backtracking process can be repeated for several root locations and types
to produce a set of low-cost pose configurations for each frame pair; having
several such configurations is important during the sequence stitching stage of
the pipeline, as explained in Section~\ref{sec:stitching}.

After determining a location and biposelet type for each subpose, a location can
be recovered for each joint in the original pose model using the stored joint
offsets associated with each assigned biposelet. In cases where two subposes
share a joint, the final joint location is the average of the joint locations
predicted by the biposelets associated with those two subposes.

\section{Sequence stitching}\label{sec:stitching}

Evaluating the first stage of the pipeline on an entire video produces a small
candidate set of pose pairs for each frame pair. The second stage of the
pipeline attempts to pick a single pose pair from each candidate set of pose
pairs, then combines poses from overlapping pairs to obtain a single pose for
each frame. Chosen pose pairs should meet two criteria:
%
\begin{description}
\item[Individual plausibility] Low cost poses, in the sense of the cost
function (\ref{eqn:full-cost}) for the frame-pair model, should be preferred over
high cost ones.
\item[Temporal consistency] Where two selected pose pairs overlap on a frame,
the poses associated with the shared frame should be similar.
\end{description}

The pair selection criteria are easy to express mathematically. For a sequence
of $F$ frames, let $\mathcal P_f$ denote the set of pose pairs detected between
frame $f$ and $f + 1$, where $f \in \{1, \ldots, F - 1\}$. Further, let $(\mat
p_{f,1}, \mat p_{f,2})$ denote the specific pose pair which the sequence
stitcher chooses from set $\mathcal P_f$, and $c_f$ be cost
(\ref{eqn:full-cost}) associated with the subpose locations and types defining
that pose pair. The objective of the stitcher is to find a sequence of pose
pairs $(\mat p_{1,1}, \mat p_{1,2}), (\mat p_{2,1}, \mat p_{2,2}), \ldots, (\mat
p_{f-1,1}, \mat p_{f-1,2})$ from the sets $\mathcal P_1, \mathcal P_2, \ldots,
\mathcal P_{F-1}$ which minimises
%
\begin{equation}\label{eqn:stitch-cost}
\sum_{f=1}^{F-1} \|\mat p_{f,2} - \mat p_{f+1,1}\|_2^2
+ \lambda \sum_{f=1}^{F-1} c_f~.
\end{equation}
%
The first term encourages temporal consistency, whilst the second favours
low-cost pose pairs over high-cost ones. $\lambda$ is a constant which balances
the two considerations.

Once an appropriate sequence of pose pairs has been chosen, a final pose may be
produced for each frame frame $1 < f < F$ by averaging $\mat p_{f-1,2}$ and
$\mat p_{f,1}$. In the first and last frames, there will only be one selected
pose to begin with, as there is only one pose pair associated with each of those
frames.

The stitching cost (\ref{eqn:stitch-cost}) can be efficiently minimised using a
dynamic programming approach analogous to the Viterbi algorithm. If there are
$|\mathcal P|$ pose pairs in each candidate set, then this maximisation will
take $\mathcal O(|\mathcal P|^2)$ time. Thus, it is important to produce only a
small selection of the best pose pairs during inference on the frame pair model.
In practice, setting $|\mathcal P|$ between 100 and 1000 generally gave an ample
selection of poses without imposing too great a computational burden.

% XXX: This is your bookmark. You still need to proof-read below here.
\section{Learning}\label{sec:learning}

The full pipeline contains a number of learnable parameters, including CNN
weights, biposelet centroids, and weights for the frame-pair cost in
(\ref{eqn:full-cost}); this section describes how each set of parameters is
learnt.

\paragraph{CNN weights} The CNN is trained to predict the joint
biposelet-subpose distribution for fixed-size crops of the training data with a
standard softmax classification objective. Positive (subpose-containing) crops
are taken around ground truth subposes in the training set, with a small margin
around the subpose to ensure that all joints are in view. Negative (background)
crops are initially taken from regions of frame pairs in the training set not
containing any people.

Training experience showed that using only pure background crops as negatives
left the CNN unable to discriminate between subposes in the centre of the
receptive field and subposes at the edges. That can significantly harm
localisation accuracy, as the CNN can predict certain biposelets with high
confidence even when the true centre of the biposelet lies far from the centre
of the receptive field. To rectify this issue, the CNN training code also
produces more challenging negative crops which include off-centre subposes that
do not correspond well (in terms of $L_2$ distance) to any of the learnt
biposelets.

Past work has shown that two-stream CNNs are prone to
overfitting~\cite{wang2015towards}; in this work, overfitting is addressed with
aggressive use of dropout and dataset augmentation, including random rotations,
flips, and small translations. Random scaling was not found to improve network
performance, and the use of large random translations is precluded by the need
to keep an entire subpose in view for the sake of accurate biposelet
prediction.

To improve convergence speed, the network is initialised with the weights of a
VGGNet trained for ILSVRC classification. The same weights are applied to both
the flow and RGB streams of the network, with filters for the RGB input layer
duplicated to accommodate the change from three image input channels to six.

\paragraph{Biposelet centroids} While writing a positive training sample for the
CNN, the training code also writes out locations of each joint relative to the
crop used to produce the sample. After all positives have been written, a set of
biposelets can be produced for each subpose by clustering those crop-relative
coordinates using $K$-means. The clustering process also yields a biposelet type
for each previously written training sample, which is then used as a target
label for the CNN\@. Clustering on the same samples used to train the CNN ensures
that the learnt biposelets reflect the range of augmentations applied to that
data, and makes it less likely that some biposelets will be underrepresented (or
not represented at all) in the CNN training set.

\paragraph{Weights for frame-pair model} The frame-pair cost
(\ref{eqn:full-cost}) can be written as an inner product between a feature
vector comprised of deformation and appearance terms and a weight vector
composed of the bias $w_0$, the deformation parameters $\{w_{s_1 s_2} : s_1, s_2
\in \mathcal S\}$ and the appearance parameters $\{w_s : s \in \mathcal S\}$.
Hence, it is possible to learn the weights using a structural SVM formulation.

Concretely, the SVM is initialised with an intuitively reasonable set of
weights, which are used to perform pose estimation over the entire training set.
This produces a set of positive feature vectors; negative feature vectors are
produced by applying the same procedure to images from the human-free portion of
the INRIA Person dataset~\cite{dalal2005histograms}. The SVM can then be trained
to label the positive feature vectors with $+1$ and the negatives with $-1$.

Minimisation of the SVM objective was performed using an existing dual
coordinate descent solver~\cite{ramanan2013dual}; one iteration of this
detect-and-optimise process was found to be sufficient to train the model. In
contrast to the experience of Chen et al.~\cite{chen2014articulated}, who used a
similar learning formulation, location and type supervision were not found to
improve the final weights learnt by the SVM.

\paragraph{Stitching parameters} In principle, the $\lambda$ used to balance
appearance and temporal consistency during pose sequence stitching can be also
be learnt using a simple global optimisation method like randomised search. In
practice, stitching performance was largely invariant to choices of $\lambda$
within several orders of magnitude of the (presumed) global optimum. As a
result, the values of $\lambda$ used in Section~\ref{sec:experiments} were
simply chosen by eyeballing the datasets in question.

\section{Experiments}\label{sec:experiments}

% Things I want to include:
% - Some sort of performance evaluation. Probably just measure eval time on
%   Poses in the Wild, or perhaps time-per-frame-pair for each pair in each of
%   the datasets, then mean recombination time per frame. Over all the test
%   sequences.

To evaluate the effectiveness of our model, we apply it to three continuous pose
estimation datasets: Poses in the Wild, MPII Cooking Activities and Human3.6M.
The following parameters were used for all datasets:

\begin{enumerate}
% Consider putting in initial values for appearance and deformation terms (in
% SVM)
\item Seven subposes were used by the frame pair model: one for each forearm
(one left and one right), one for each elbow, one for each upper arm, and one
root subpose containing both shoulders.
\item $K = 100$ biposelet types were learnt for each subpose.
\item $|\mathcal P| = 300$ candidate pose pairs were extracted from each frame
pair.
\item $\lambda = 10^5$ was used during stitching.
\end{enumerate}

To measure localisation error, we employ variants on Percentage Correct
Keypoints (PCK) and Percentage Correct Parts (PCP), as explained by Yang \&
Ramanan~\cite{yang2013articulated}. Except where hip and shoulder locations are
available for normalisation, we use unnormalised PCK: that is, we consider a
joint to be correctly localised for the purpose of PCK calculation if its
predicted position in the image falls within a certain number of pixels of the
ground truth location. For PCP, we consider a limb of length $l$ to be correctly
localised at a threshold $t$ if both predicted endpoints fall within a distance
$tl$ of their true locations; in the case of $t = 0.5$, this is simply ordinary
strict PCP.

Code for all experiments is available
online.\footnote{\url{https://github.com/qxcv/joint-regressor}} Note that all
experiments were performed on a machine with 128GB of memory, 12 physical CPU
cores, and GPUs with 12GB of video memory each; a similarly specced machine will
be required to train and evaluate the convolutional networks used here.

\subsection{Datasets}

\begin{figure*}[t]
\begin{center}
\input{figures/plots/piw-pck.pgf}
\end{center}
\vspace{-8mm}
\caption{Unnormalised PCK on Poses in the Wild. The curves for Chen \& Yuille
and the combined method occlude one another in the shoulder plot.}
\label{fig:piw-pcks}
\end{figure*}

\begin{table}[t]
{\footnotesize\tabulinesep=1mm
\begin{tabu} to \textwidth {X[2l] || X[c]X[c]X[c] | X[c]X[c]X[c]}
& \multicolumn{3}{c|}{Upper arms} & \multicolumn{3}{c}{Lower arms}\\
PCP threshold & 0.3 & 0.5 & 0.8 & 0.3 & 0.5 & 0.8\\
\tabucline-
Chen \& Yuille~\cite{chen2014articulated} &
32.31\% & 75.91\% & 94.21\% & 48.30\% & 73.55\% & 85.19\%\\
Cherian et al.~\cite{cherian2014mixing} &
27.66\% & 57.14\% & 78.04\% & 30.26\% & 50.32\% & 61.44\%\\
Combined~\cite{cherian2014mixing} and~\cite{chen2014articulated} &
31.63\% & 76.91\% & 95.02\% & 50.94\% & 76.29\% & 87.76\%\\
Pfister et al. SpatialNet~\cite{pfister2015flowing} &
51.89\% & 72.50\% & 83.11\% & 36.90\% & 54.36\% & 65.37\%\\
Ours &
59.46\% & 81.88\% & 90.17\% & 31.97\% & 52.25\% & 69.03\%\\
\end{tabu}}\\
\caption{PCP at various thresholds on Poses in the Wild.}
\label{tbl:piw-pcps}
\end{table}

\paragraph{FLIC and PIW} Poses in the Wild (PIW)~\cite{cherian2014mixing} is a
video pose estimation dataset of with frames drawn from Hollywood movies. Scene
clutter, camera motion, subject occlusion and rapid subject motion all make the
dataset a challenging benchmark.

Unfortunately, at under a thousand frames, Poses in the Wild is not large enough
to both test and evaluate on. Thus, we follow the example of previous authors in
training our model on the Frames Labelled in Cinema (FLIC)
dataset~\cite{sapp2013modec} and then evaluating on PIW\@. As we require
adjacent labelled frames to train our frame-pair model, we used a subset of
FLIC-full for training which consisted of around 8000 reliably annotated pairs.
The remaining pairs had a mixture of incorrect annotations and excessive
occlusion which made them unsuitable for training our appearance model.

All baselines used for comparison were produced using publicly released
evaluation code and models trained on FLIC in a similar manner to ours. Note
that the ``combined'' baseline works by applying Chen \&
Yuille's~\cite{chen2014articulated} detector to each frame of a video sequence,
then combining the produced candidate pose sets for each frame into a pose
sequence using the recombination method of Cherian et
al.~\cite{cherian2014mixing}. It should also be noted that the Pfister et al.\
baseline is for a non-temporally-aware model, which they dub ``SpatialNet'',
rather than the flow-augmented model for which they obtained the best results;
at the time of writing, code for the latter model was not publicly available.

PCK at image scale over all of Poses in the Wild is shown in
Figure~\ref{fig:piw-pcks}, while PCP at various thresholds is shown in
Table~\ref{tbl:piw-pcps}.

\begin{figure*}[b]
\begin{center}
\input{figures/plots/mpii-pck.pgf}
\end{center}
\vspace{-8mm}
\caption{Unnormalised PCK on MPII Cooking Activities.}
\label{fig:mpii-pcks}
\end{figure*}

\begin{table}
{\footnotesize\tabulinesep=1mm
\begin{tabu} to \textwidth {X[2l] || X[c]X[c]X[c] | X[c]X[c]X[c]}
& \multicolumn{3}{c|}{Upper arms} & \multicolumn{3}{c}{Lower arms}\\
PCP threshold & 0.3 & 0.5 & 0.8 & 0.3 & 0.5 & 0.8\\
\tabucline-
Chen \& Yuille~\cite{chen2014articulated} &
79.44\% & 95.07\% & 98.39\% & 80.03\% & 96.00\% & 97.90\%\\
Cherian et al.~\cite{cherian2014mixing} &
67.97\% & 80.91\% & 88.62\% & 55.71\% & 75.93\% & 83.45\%\\
Combined~\cite{cherian2014mixing} and~\cite{chen2014articulated} &
79.79\% & 95.07\% & 98.58\% & 81.54\% & 96.97\% & 99.07\%\\
Pfister et al. SpatialNet~\cite{pfister2015flowing} &
1.52\% & 28.93\% & 57.18\% & 0.29\% & 14.37\% & 70.97\%\\
Ours &
76.59\% & 87.44\% & 94.72\% & 69.21\% & 85.43\% & 94.28\%\\
\end{tabu}}\\
\caption{PCP at various thresholds on MPII Cooking Activities.}
\label{tbl:mpii-pcps}
\end{table}

\paragraph{MPII Cooking Activities} MPII Cooking
Activities~\cite{rohrbach2012database} includes two pose estimation datasets
which we use for comparison with existing work. The Cooking Activities data
represents a near-ideal case for video pose estimation: all videos are recorded
from the same static camera in the same kitchen, with minimal occlusion and
only one subject per video sequence.

We train our model on the continuous pose estimation dataset released with MPII
Cooking Activities, while using the training set for the Cooking Activities
``pose challenge'' to evaluate both our model and the baselines. It should be
noted that we are evaluating on the \textit{training} set of the pose challenge
as the test set is not continuous; despite the confusing nomenclature, the
training set of the pose challenge is not the same as the continuous pose
estimation dataset.

As with the comparison on PIW, all baselines used here were produced using
publicly released models trained for the FLIC dataset. However, it should be
noted that we had significant difficulty getting Pfister et al.'s publicly
released SpatialNet model~\cite{pfister2015flowing} to produce sensible results
on MPII Cooking Activities. This may have been due to a sub-optimal
cropping-and-scaling strategy or some other mistake on our part, and so the
results should not be treated as being representative of the maximal performance
of their approach.

PCK curves for MPII Cooking Activities are given in Figure~\ref{fig:mpii-pcks},
and PCP various thresholds is given in Table~\ref{tbl:mpii-pcps}.

\begin{figure*}[t]
\begin{center}
\input{figures/plots/h36m-pck.pgf}
\end{center}
\vspace{-8mm}
\caption{Comparison of PCK for subject five on the Human3.6M dataset. Thresholds
are expressed as fractions of the distance between the subject's left hip and
right shoulder.}
\label{fig:h36m-pck}
\end{figure*}

\paragraph{Human3.6M} Human3.6M~\cite{ionescu2014human,ionescu2011latent} is a
pose estimation and action recognition dataset which includes full depth and 3D
pose data recorded with a motion capture system, although we evaluate only on
the RGB video and 2D pose portions of the dataset. As a motion capture dataset,
Human3.6M is recorded in a controlled environment with a uniform background and
no camera motion. However, in other respects it is significantly more
challenging than PIW or MPII Cooking Activities: not only do its actors exhibit
a wider range of motion, but all motion is recorded from four cameras spaced
evenly around the scene, meaning that a significant portion of poses are
non-frontal.

Our primary motivation in evaluating on Human3.6M is to compare to the recent
results reported by Fragkiadaki et al.~\cite{fragkiadaki2015recurrent} for a
sophisticated recurrent neural network architecture. We adopt a comparable
training protocol to Fragkiadaki et al.\ by using subject five for evaluation and
all other subjects for training. Due to time constraints, we were unable to
evaluate our model on all frames associated with subject five, and had to settle
for a randomly selected set of half the 120 available scenes, each of which was
further trimmed to 5\% of its original length. PCK for both our model and the
recurrent network approach are given in Figure~\ref{fig:h36m-pck}.

\subsection{Discussion}\label{sec:discussion}

% General points I want to make (removing as I make them):
% - Downsampling due to convolutional neural network may be to blame for a lot
%   of errors. In principle, it should hurt accuracy quite a bit. However, it's
%   not easy to fix, since upsampling creates unmanageably large pixel volumes.
% - Cost function for neural network also means that the network can't make use
%   of contextual information beyond a small receptive field; this makes it hard
%   to come up with good appearance terms where a subpose is ambiguous.

\begin{figure*}[t]
\begin{center}
\begin{tabular}{@{}c@{}c c@{}c@{}c@{}}
\multicolumn{2}{c}{Localisation successes} &
\multicolumn{3}{c}{Localisation failures}\\[0.4em]
\multicolumn{5}{c}{Poses in the Wild}\\
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/piw-s18f30-good-gump-mower.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/piw-s20f2-good-suit.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/piw-s9f18-bad-no-wrist.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/piw-s10f27-bad-pose-transfer.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/piw-s13f15-bad-terminal-movement.jpg}\\
(a) & (b) & (c) & (d) & (e)\\[0.6em]
\multicolumn{5}{c}{MPII Cooking Activities}\\
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/mpii-s10f54-good-white-shirt.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/mpii-s8f38-good-open-draw.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/mpii-s2f10-bad-shoulders.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/mpii-s5f52-bad-shoulders.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/mpii-s14f21-bad-forearm.jpg}\\
(f) & (g) & (h) & (i) & (j)\\[0.6em]
\multicolumn{5}{c}{Human3.6M}\\
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/h36m-s28f2-good-teapot.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/h36m-s31f12-good-chair.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/h36m-s7f8-bad-extreme-articulation.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/h36m-s24f12-bad-flip-1.jpg}\,&
\includegraphics[width=0.17\linewidth]{figures/shots-cropped/h36m-s24f14-bad-flip-2.jpg}\\
(k) & (l) & (m) & (n) & (o)
\end{tabular}
\end{center}
\vspace{-6mm}
\caption{Characteristic successes (first two columns) and failures (last three
columns).}
\label{fig:qualitative}
\end{figure*}

The results for Cooking Activities and PIW show that our model improves
significantly on past work in the localisation of shoulders, and is competitive
on elbow localisation; this is reflected in both PCKs and in the difference
between upper arm and forearm PCPs. It's likely that the use of subposes is
responsible for our high performance on shoulders. As mentioned in
Section~\ref{sec:intro}, localising an entire subpose can be easier than
localising a single joint---as all baseline methods do---because the CNN is
able to make use of more surrounding context. This is particularly useful for
shoulders, since the upper part of the torso is almost always in view whenever
both shoulders are, the torso's size and consistent appearance make it easy to
identify~\cite{mori2004recovering}. As is explained later in this section, arms
likely do not benefit from the use of subposes to the same degree as shoulders
because of their higher level of articulation.

Despite our model's excellent overall performance in localising shoulders,
qualitative analysis revealed that shoulders were very poorly localised in a
small subset of the Cooking Activities frames. Frames (h) and (i) in
Figure~\ref{fig:qualitative} illustrate this problem. The fact that the same
issue does not crop up in the more challenging PIW dataset suggests that our
training protocol for Cooking Activities may be flawed. A possible culprit is
the lack of diversity in the subset of Cooking Activities used for
training---Cooking Activities uses only a handful of actors, so our CNN may be
overfitting to aspects of their appearance when learning to classify shoulders.
This problem may be ameliorated by training on FLIC, as was done for the PIW
evaluation, rather than on a subset of Cooking Activities.

A major weakness of our model across all datasets was in handling high levels of
joint articulation. Nowhere was this more evident than in the wrists---examples
(e), (j) and (m) in Figure~\ref{fig:qualitative} each depict forearms which are
unusual for their respective datasets, and are representative of the many wrist
localisation failures observed during evaluation. Such failures may be due to
the limited number of configurations which can be expressed using biposelets;
the baseline models do not suffer from this problem to the same degree because
they localise individual joints directly, rather than localising entire subposes
and then attempting to extract specific joint locations using a discrete set of
subpose configurations.

Biposelets must be able to express both joint location and motion, so it may be
that biposelets which express a high degree of motion are less able to express
fine differences in location. That could partially explain why forearms are
localised so poorly relative to upper arms: wrists and elbows undergo a higher
degree of motion than shoulders, so it would be expected that their associated
biposelets would be more specialised to motion than location. Experimenting with
higher numbers of biposelets and more subposes along the forearms is a promising
avenue for future work on this deficiency of the model.

Another possibility is that optical flow is not being used as effectively as it
could be. Wrists can move much faster than elbows or shoulders, so the magnitude
of optical flow in an image patch should be a strong cue for the appearance of
wrists. This is clearly illustrated by the ``combined'' baseline in
Figure~\ref{fig:piw-pcks} and Figure~\ref{fig:mpii-pcks}: combining Chen \&
Yuille's approach with Cherian et al.'s (largely flow-based) recombination
heuristics yields the greatest performance improvement for wrists, and no
performance improvement for shoulders. Although our model uses raw optical flow
at the input layer of the CNN, Jain et al.~\cite{jain2014modeep} suggest that
use of raw flow can lead CNNs to overfit, and that supplying only the magnitude
of flow may be more effective.

Much like high level of joint articulation, occlusion was a significant
challenge to our model. Frames (c) and (d) in Figure~\ref{fig:qualitative} show
occlusion-related failures: in (c), the presence of a flame occluding the
subject's left wrist has led their entire forearm to be improperly localised,
whilst in (d), two subjects occlude one another, and the detector attempts to
fit the same pose to both of them.

Poor performance on occlusions could be due to a flaw of our model: apart from
the pairwise deformation features in the graphical model used for frame pair
inference, our model does not have any way of reasoning about the position and
type of an occluded subpose based on the appearance of its visible neighbours.
Improving performance on subposes which are entirely occluded will likely
require the use introduction of something like Chen \& Yuille's image-dependent
pairwise relations~\cite{chen2014articulated}, which enable reasoning about the
position of a joint (or subpose, in our case) based on actual image evidence at
neighbouring joints (subposes).

Localisation performance on Human3.6M was uniformly poor relative to the
baseline. Visual inspection revealed that a majority of errors were due to
confusion between the left and right arms. For example, poses (n) and (o) in
Figure~\ref{fig:qualitative} show a situation in which the estimator reversed
its labelling of the left and right arms within the space of two frames, even
though the subject remained facing in the same direction. Moreover, left--right
confusion also manifested itself in instances like (m), where both the left and
right arm were predicted to lie in the same place.

Left--right confusion is a common problem in pose estimation; indeed,
Fragkiadaki et al.~\cite{fragkiadaki2015recurrent} note that their recurrent
model outperforms a per-frame baseline precisely because it is so effective at
resolving left--right confusion. As future work, it may be useful to augment our
per-frame graphical model with a global latent variable indicating whether a
person is forward-facing or backwards-facing, as Sapp \& Taskar
do~\cite{sapp2013modec} to eliminate left--right confusion.

\section{Conclusion}

Biposelets have proved to be effective in boosting pose estimation performance
on shoulders and, to a lesser extent, elbows. A biposelet-based approach is able
to make effective use of the distinctive visual context of those joints by
localising entire subposes instead of identifying a single joint at a time.
Predicting poses in pairs also leads to an elegant formulation for stitching
pose predictions together in a video setting, and forces our model to make use
of the visual context present in two video frames at a time. The price of
these practical and theoretical improvements has been a drop in localisation
accuracy for faster-moving joints; this likely stems from the limited ability of
biposelets to represent both the motion and the position of those joints.
Nevertheless, the already-competitive performance of our approach and the
opportunities for improvement enumerated in Section~\ref{sec:discussion} make
biposelets a promising area for future research.

\ifaccvfinal
\section*{Acknowledgments}

I would like to thank various authors whose code I have used to produce baseline
comparisons~\cite{pfister2015flowing,chen2014articulated,cherian2014mixing}. I
would also like to thank Anoop Cherian for informing many of the ideas presented
and giving extensive feedback on drafts of this paper.
\fi

%===========================================================
\bibliographystyle{splncs}
\bibliography{citations}
\end{document}
